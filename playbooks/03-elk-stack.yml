---
- name: Deploy ELK Stack on Data Services Server
  hosts: data_services
  become: yes
  vars:
    elasticsearch_version: "8.11.0"
    elasticsearch_heap_size: "4g"
  tasks:
    - name: Create ELK directory structure
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
      loop:
        - /opt/elk
        - /opt/elk/elasticsearch/data
        - /opt/elk/elasticsearch/logs
        - /opt/elk/elasticsearch/config
        - /opt/elk/logstash/config
        - /opt/elk/logstash/pipeline
        - /opt/elk/kibana/config

    - name: Create Elasticsearch configuration
      copy:
        content: |
          cluster.name: forensics-cluster
          node.name: forensics-node-1
          path.data: /usr/share/elasticsearch/data
          path.logs: /usr/share/elasticsearch/logs
          network.host: 0.0.0.0
          http.port: 9200
          discovery.type: single-node
          xpack.security.enabled: false
          xpack.security.http.ssl.enabled: false
          xpack.security.transport.ssl.enabled: false
        dest: /opt/elk/elasticsearch/config/elasticsearch.yml

    - name: Create Logstash configuration
      copy:
        content: |
          path.data: /usr/share/logstash/data
          pipeline.workers: 4
          pipeline.batch.size: 125
          pipeline.batch.delay: 50
          path.config: /usr/share/logstash/pipeline
          path.logs: /usr/share/logstash/logs
          xpack.monitoring.enabled: false
        dest: /opt/elk/logstash/config/logstash.yml

    - name: Create Logstash pipeline configuration
      copy:
        content: |
          input {
            file {
              path => "/data/processed/autopsy/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "autopsy"
              tags => ["forensics", "autopsy"]
            }
            file {
              path => "/data/processed/volatility/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "volatility"
              tags => ["forensics", "memory"]
            }
            file {
              path => "/data/processed/andriller/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "andriller"
              tags => ["forensics", "mobile"]
            }
            file {
              path => "/data/processed/cape/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "cape"
              tags => ["forensics", "malware"]
            }
            beats {
              port => 5044
            }
          }

          filter {
            # Add timestamp processing
            mutate { 
              add_field => { "timestamp" => "%{@timestamp}" } 
            }

            # Parse Autopsy data
            if [type] == "autopsy" {
              if [timestamp] {
                date {
                  match => [ "timestamp", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
                  target => "@timestamp"
                }
              }
              
              if [file_path] {
                grok {
                  match => { "file_path" => "(?<file_directory>.*)/(?<file_name>[^/]+)$" }
                }
                
                grok {
                  match => { "file_name" => ".*\.(?<file_extension>[^.]+)$" }
                }
              }
            }

            # Parse Volatility data
            if [type] == "volatility" {
              if [process_name] and [pid] {
                mutate {
                  add_field => { "process_key" => "%{process_name}-%{pid}" }
                }
              }
            }

            # Parse mobile data
            if [type] == "andriller" {
              if [app_name] {
                mutate {
                  add_field => { "device_type" => "mobile" }
                }
              }
            }

            # Parse malware analysis
            if [type] == "cape" {
              if [malware_family] {
                mutate {
                  add_field => { "threat_type" => "malware" }
                }
              }
            }

            # Add case metadata
            if [case_id] {
              mutate {
                add_field => { "lab_processed" => true }
                add_field => { "processing_time" => "%{@timestamp}" }
              }
            }
          }

          output {
            elasticsearch {
              hosts => ["elasticsearch:9200"]
              index => "forensics-%{type}-%{+YYYY.MM.dd}"
            }
            
            stdout {
              codec => rubydebug
            }
          }
        dest: /opt/elk/logstash/pipeline/forensics.conf

    - name: Create Docker Compose for ELK Stack
      copy:
        content: |
          services:
            elasticsearch:
              image: docker.elastic.co/elasticsearch/elasticsearch:{{ elasticsearch_version }}
              container_name: elasticsearch
              environment:
                - discovery.type=single-node
                - "ES_JAVA_OPTS=-Xms{{ elasticsearch_heap_size }} -Xmx{{ elasticsearch_heap_size }}"
                - xpack.security.enabled=false
                - xpack.security.http.ssl.enabled=false
                - xpack.security.transport.ssl.enabled=false
                - cluster.name=forensics-cluster
                - node.name=forensics-node-1
              ports:
                - "9200:9200"
                - "9300:9300"
              volumes:
                - ./elasticsearch/data:/usr/share/elasticsearch/data:Z
                - ./elasticsearch/logs:/usr/share/elasticsearch/logs:Z
                - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
              restart: unless-stopped
              ulimits:
                memlock:
                  soft: -1
                  hard: -1
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
                interval: 30s
                timeout: 10s
                retries: 5

            logstash:
              image: docker.elastic.co/logstash/logstash:{{ elasticsearch_version }}
              container_name: logstash
              volumes:
                - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
                - ./logstash/pipeline:/usr/share/logstash/pipeline
                - /data:/data:ro
              ports:
                - "5044:5044"
                - "5000:5000/tcp"
                - "5000:5000/udp"
                - "9600:9600"
              environment:
                LS_JAVA_OPTS: "-Xmx1g -Xms1g"
              depends_on:
                - elasticsearch
              restart: unless-stopped

            kibana:
              image: docker.elastic.co/kibana/kibana:{{ elasticsearch_version }}
              container_name: kibana
              ports:
                - "5601:5601"
              environment:
                ELASTICSEARCH_HOSTS: http://elasticsearch:9200
                SERVER_NAME: forensics-kibana
                SERVER_HOST: 0.0.0.0
              depends_on:
                - elasticsearch
              restart: unless-stopped
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
                interval: 30s
                timeout: 10s
                retries: 5

          volumes:
            elasticsearch_data:
            logstash_data:
        dest: /opt/elk/docker-compose.yml
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Set proper ownership for ELK directories
      file:
        path: /opt/elk
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        recurse: yes

    - name: Fix Elasticsearch data and logs directory permissions for Docker
      file:
        path: "{{ item }}"
        owner: "1000"
        group: "1000"
        mode: '0755'
        recurse: yes
      loop:
        - /opt/elk/elasticsearch/data
        - /opt/elk/elasticsearch/logs

    - name: Ensure Elasticsearch directories have correct SELinux context (if SELinux is enabled)
      shell: |
        if command -v semanage >/dev/null 2>&1; then
          setsebool -P container_manage_cgroup on || true
          chcon -Rt svirt_sandbox_file_t /opt/elk/elasticsearch/data || true
          chcon -Rt svirt_sandbox_file_t /opt/elk/elasticsearch/logs || true
        fi
      ignore_errors: yes

    - name: Start ELK Stack services
      command: docker-compose up -d
      args:
        chdir: /opt/elk

    - name: Wait for Elasticsearch to be ready
      uri:
        url: http://localhost:9200/_cluster/health
        method: GET
        status_code: 200
      register: elasticsearch_health
      until: elasticsearch_health.status == 200
      retries: 30
      delay: 10

    - name: Wait for Kibana to be ready
      uri:
        url: http://localhost:5601/api/status
        method: GET
        status_code: 200
      register: kibana_status
      until: kibana_status.status == 200
      retries: 30
      delay: 10

    - name: Display ELK access information
      debug:
        msg: |
          ELK Stack is now accessible at:
          Elasticsearch: http://{{ ansible_default_ipv4.address }}:9200
          Kibana: http://{{ ansible_default_ipv4.address }}:5601
          Logstash: http://{{ ansible_default_ipv4.address }}:9600
          
          External access (formie VM):
          Elasticsearch: http://34.41.228.106:9200
          Kibana: http://34.41.228.106:5601
          
          Logstash will process forensics data from /data/processed/