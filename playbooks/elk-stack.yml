---
- name: Deploy ELK Stack on Data Services Server
  hosts: data_services
  become: yes
  vars:
    elasticsearch_version: "8.11.0"
    elasticsearch_heap_size: "1g"
  tasks:
    - name: Create ELK directory structure
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
      loop:
        - /opt/elk
        - /opt/elk/elasticsearch/data
        - /opt/elk/elasticsearch/logs
        - /opt/elk/elasticsearch/config
        - /opt/elk/logstash/config
        - /opt/elk/logstash/pipeline
        - /opt/elk/kibana/config

    - name: Create Elasticsearch configuration
      copy:
        content: |
          cluster.name: forensics-cluster
          node.name: forensics-node-1
          path.data: /usr/share/elasticsearch/data
          path.logs: /usr/share/elasticsearch/logs
          network.host: 0.0.0.0
          http.port: 9200
          discovery.type: single-node
          xpack.security.enabled: false
          xpack.security.http.ssl.enabled: false
          xpack.security.transport.ssl.enabled: false
        dest: /opt/elk/elasticsearch/config/elasticsearch.yml

    - name: Create Logstash configuration
      copy:
        content: |
          path.data: /usr/share/logstash/data
          pipeline.workers: 4
          pipeline.batch.size: 125
          pipeline.batch.delay: 50
          path.config: /usr/share/logstash/pipeline
          path.logs: /usr/share/logstash/logs
          xpack.monitoring.enabled: false
        dest: /opt/elk/logstash/config/logstash.yml

    - name: Create Logstash pipeline configuration
      copy:
        content: |
          input {
            file {
              path => "/data/processed/autopsy/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "autopsy"
              tags => ["forensics", "autopsy"]
            }
            file {
              path => "/data/processed/volatility/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "volatility"
              tags => ["forensics", "memory"]
            }
            file {
              path => "/data/processed/andriller/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "andriller"
              tags => ["forensics", "mobile"]
            }
            file {
              path => "/data/processed/cape/*.json"
              start_position => "beginning"
              sincedb_path => "/dev/null"
              codec => "json"
              type => "cape"
              tags => ["forensics", "malware"]
            }
            beats {
              port => 5044
            }
          }

          filter {
            # Add timestamp processing
            mutate { 
              add_field => { "timestamp" => "%{@timestamp}" } 
            }

            # Parse Autopsy data
            if [type] == "autopsy" {
              if [timestamp] {
                date {
                  match => [ "timestamp", "yyyy-MM-dd HH:mm:ss", "ISO8601" ]
                  target => "@timestamp"
                }
              }
              
              if [file_path] {
                grok {
                  match => { "file_path" => "(?<file_directory>.*)/(?<file_name>[^/]+)$" }
                }
                
                grok {
                  match => { "file_name" => ".*\.(?<file_extension>[^.]+)$" }
                }
              }
            }

            # Parse Volatility data
            if [type] == "volatility" {
              if [process_name] and [pid] {
                mutate {
                  add_field => { "process_key" => "%{process_name}-%{pid}" }
                }
              }
            }

            # Parse mobile data
            if [type] == "andriller" {
              if [app_name] {
                mutate {
                  add_field => { "device_type" => "mobile" }
                }
              }
            }

            # Parse malware analysis
            if [type] == "cape" {
              if [malware_family] {
                mutate {
                  add_field => { "threat_type" => "malware" }
                }
              }
            }

            # Add case metadata
            if [case_id] {
              mutate {
                add_field => { "lab_processed" => true }
                add_field => { "processing_time" => "%{@timestamp}" }
              }
            }
          }

          output {
            elasticsearch {
              hosts => ["elasticsearch:9200"]
              index => "forensics-%{type}-%{+YYYY.MM.dd}"
            }
            
            stdout {
              codec => rubydebug
            }
          }
        dest: /opt/elk/logstash/pipeline/forensics.conf

    - name: Create Docker Compose for ELK Stack
      copy:
        content: |
          version: '3.8'
          services:
            elasticsearch:
              image: docker.elastic.co/elasticsearch/elasticsearch:{{ elasticsearch_version }}
              container_name: elasticsearch
              environment:
                - discovery.type=single-node
                - "ES_JAVA_OPTS=-Xms{{ elasticsearch_heap_size }} -Xmx{{ elasticsearch_heap_size }}"
                - xpack.security.enabled=false
                - xpack.security.http.ssl.enabled=false
                - xpack.security.transport.ssl.enabled=false
                - cluster.name=forensics-cluster
                - node.name=forensics-node-1
              ports:
                - "9200:9200"
                - "9300:9300"
              volumes:
                - ./elasticsearch/data:/usr/share/elasticsearch/data
                - ./elasticsearch/logs:/usr/share/elasticsearch/logs
                - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
              restart: unless-stopped
              ulimits:
                memlock:
                  soft: -1
                  hard: -1

            logstash:
              image: docker.elastic.co/logstash/logstash:{{ elasticsearch_version }}
              container_name: logstash
              volumes:
                - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
                - ./logstash/pipeline:/usr/share/logstash/pipeline
                - /data:/data:ro
              ports:
                - "5044:5044"
                - "5000:5000/tcp"
                - "5000:5000/udp"
                - "9600:9600"
              environment:
                LS_JAVA_OPTS: "-Xmx1g -Xms1g"
              depends_on:
                - elasticsearch
              restart: unless-stopped

            kibana:
              image: docker.elastic.co/kibana/kibana:{{ elasticsearch_version }}
              container_name: kibana
              ports:
                - "5601:5601"
              environment:
                ELASTICSEARCH_HOSTS: http://elasticsearch:9200
                SERVER_NAME: forensics-kibana
                SERVER_HOST: 0.0.0.0
              depends_on:
                - elasticsearch
              restart: unless-stopped

          volumes:
            elasticsearch_data:
            logstash_data:
        dest: /opt/elk/docker-compose.yml
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"

    - name: Set proper ownership for ELK directories
      file:
        path: /opt/elk
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        recurse: yes

    - name: Set proper ownership for Elasticsearch data and logs directories
      file:
        path: "{{ item }}"
        owner: "1000"
        group: "1000"
        recurse: yes
        state: directory
      loop:
        - /opt/elk/elasticsearch/data
        - /opt/elk/elasticsearch/logs

    - name: Set proper ownership for Logstash directories
      file:
        path: "{{ item }}"
        owner: "1000"
        group: "1000"
        recurse: yes
        state: directory
      loop:
        - /opt/elk/logstash

    - name: Set proper ownership for Kibana directories
      file:
        path: "{{ item }}"
        owner: "1000"
        group: "1000"
        recurse: yes
        state: directory
      loop:
        - /opt/elk/kibana

    - name: Set proper permissions for ELK directories
      file:
        path: "{{ item }}"
        mode: '0755'
        recurse: yes
      loop:
        - /opt/elk/elasticsearch
        - /opt/elk/logstash
        - /opt/elk/kibana

    - name: Install Docker and Docker Compose
      apt:
        name:
          - docker.io
          - docker-compose
        state: present
        update_cache: yes

    - name: Start and enable Docker service
      systemd:
        name: docker
        state: started
        enabled: yes

    - name: Add user to docker group
      user:
        name: "{{ ansible_user }}"
        groups: docker
        append: yes

    - name: Deploy ELK Stack with Docker Compose
      shell: |
        cd /opt/elk
        docker-compose down || true
        docker-compose up -d
      become_user: "{{ ansible_user }}"

    - name: Wait for Elasticsearch to be ready
      uri:
        url: "http://localhost:9200/_cluster/health"
        method: GET
        timeout: 300
      register: elasticsearch_health
      until: elasticsearch_health.status == 200 and elasticsearch_health.json.status in ["yellow", "green"]
      retries: 30
      delay: 10

    - name: Wait for Kibana to be ready
      uri:
        url: "http://localhost:5601/api/status"
        method: GET
        timeout: 300
      register: kibana_health
      until: kibana_health.status == 200 and kibana_health.json.status.overall.level == "available"
      retries: 30
      delay: 10

    - name: Display ELK Stack access information
      debug:
        msg: |
          ELK Stack is now running and accessible:
          
          Elasticsearch: http://{{ ansible_default_ipv4.address }}:9200
          Kibana: http://{{ ansible_default_ipv4.address }}:5601
          Logstash: Listening on port 5044 for Beats input
          
          You can now:
          1. Access Kibana in your browser for data visualization
          2. Send forensics data to Logstash for processing
          3. Query Elasticsearch directly for raw data access